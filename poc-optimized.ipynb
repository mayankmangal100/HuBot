{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8207c688-7b95-4923-a54f-26d02dedb23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enterprise Conversational RAG System\n",
    "- Low-latency (<15s on CPU)\n",
    "- Modular design for easy component replacement\n",
    "- Comprehensive logging for performance monitoring\n",
    "- Hybrid retrieval (BM25 + Embeddings)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d387e11-c6b6-4b6b-b436-0c2077138d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"rag_system.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"rag_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e380e5-2fa2-48ed-a66b-f13a827433d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tracking\n",
    "@dataclass\n",
    "class LatencyTracker:\n",
    "    start_time: float = 0\n",
    "    \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def end(self, operation_name: str) -> float:\n",
    "        duration = time.time() - self.start_time\n",
    "        logger.info(f\"LATENCY: {operation_name} took {duration:.4f} seconds\")\n",
    "        return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ce34a-293f-4ed1-9070-9061d60f1247",
   "metadata": {},
   "source": [
    "##### Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c1d688-7966-44d0-97d2-510b6a544aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading and extraction from various formats\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            from unstructured.partition.pdf import partition_pdf\n",
    "            self.partition_pdf = partition_pdf\n",
    "        except ImportError:\n",
    "            logger.error(\"unstructured package not installed. Install with: pip install unstructured pdf2image pytesseract\")\n",
    "            raise\n",
    "    \n",
    "    def process_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF files\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        try:\n",
    "            elements = self.partition_pdf(file_path)\n",
    "            text = \"\\n\\n\".join([str(element) for element in elements])\n",
    "            \n",
    "            tracker.end(f\"PDF processing ({os.path.basename(file_path)})\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {file_path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d61ea-ae30-40a9-adaf-c42ea103af15",
   "metadata": {},
   "source": [
    "##### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983d1cec-5e98-44ec-b0c1-3ca5f8a0781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    \"\"\"Handles document chunking using various strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        try:\n",
    "            from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "            self.splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "        except ImportError:\n",
    "            logger.error(\"langchain_text_splitters not installed. Install with: pip install langchain-text-splitters\")\n",
    "            raise\n",
    "    \n",
    "    def split_text(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into chunks with metadata\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "            \n",
    "        chunks = self.splitter.create_documents([text], [metadata])\n",
    "        chunks_with_metadata = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunks_with_metadata.append({\n",
    "                \"id\": f\"{metadata.get('source', 'doc')}_{i}\",\n",
    "                \"text\": chunk.page_content,\n",
    "                \"metadata\": {**chunk.metadata, \"chunk_id\": i}\n",
    "            })\n",
    "        \n",
    "        tracker.end(f\"Text splitting ({len(chunks_with_metadata)} chunks)\")\n",
    "        return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfec665-1dd3-4dbc-986b-b24a397d95a8",
   "metadata": {},
   "source": [
    "##### EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decabb71-2b75-406b-9a78-68cb47ac255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Optimized embedding model with caching and quantization\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-base-en-v1.5\", device: str = None):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Determine device - prefer CUDA, fall back to CPU\n",
    "        # if device is None:\n",
    "        #     self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # else:\n",
    "        self.device = \"cpu\"\n",
    "            \n",
    "        logger.info(f\"Using device: {self.device} for embeddings\")\n",
    "        \n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import torch\n",
    "            \n",
    "            tracker = LatencyTracker().start()\n",
    "            \n",
    "            # Optimize for CPU inference\n",
    "            torch.set_num_threads(max(1, multiprocessing.cpu_count() - 1))\n",
    "            \n",
    "            # Use smaller model that's faster but still accurate\n",
    "            # Options: \"BAAI/bge-small-en-v1.5\" (faster) or \"intfloat/e5-small-v2\" (very fast)\n",
    "            self.model = SentenceTransformer(model_name, device=self.device)\n",
    "            \n",
    "            # Quantize model for faster inference if on CPU\n",
    "            if self.device == \"cpu\":\n",
    "                try:\n",
    "                    logger.info(\"Applying dynamic quantization to embedding model\")\n",
    "                    self.model.model = torch.quantization.quantize_dynamic(\n",
    "                        self.model.model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Quantization failed: {e}\")\n",
    "            \n",
    "            tracker.end(f\"Loading optimized embedding model {model_name}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"sentence_transformers not installed. Install with: pip install sentence-transformers\")\n",
    "            raise\n",
    "    \n",
    "    # Add LRU cache to avoid re-computing embeddings for the same text\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def _embed_single_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text with caching\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model.encode(text)\n",
    "            \n",
    "        if isinstance(embedding, torch.Tensor):\n",
    "            embedding = embedding.cpu().numpy().tolist()\n",
    "            \n",
    "        return embedding\n",
    "    \n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a list of texts\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        # Process in optimal batch size\n",
    "        batch_size = 8  # Smaller batch size for CPU\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Use thread pool for parallel processing on CPU\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            # Submit tasks in batches\n",
    "            futures = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                futures.append(executor.submit(self._batch_embed, batch))\n",
    "                \n",
    "            # Collect results\n",
    "            for future in futures:\n",
    "                all_embeddings.extend(future.result())\n",
    "        \n",
    "        tracker.end(f\"Embedding {len(texts)} texts\")\n",
    "        return all_embeddings\n",
    "    \n",
    "    def _batch_embed(self, batch: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Process a batch of embeddings\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.encode(batch, convert_to_tensor=True)\n",
    "            embeddings = embeddings.cpu().numpy().tolist()\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single query\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        result = self._embed_single_text(query)\n",
    "        tracker.end(\"Query embedding\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4f361-8b03-4cba-a6c9-1f84cb807e0b",
   "metadata": {},
   "source": [
    "##### VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0012465-998e-4419-80b5-65444e982c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Handles vector storage and retrieval using Qdrant\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"documents\", vector_size: int = 384):\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_size = vector_size\n",
    "        \n",
    "        try:\n",
    "            from qdrant_client import QdrantClient\n",
    "            from qdrant_client.http import models\n",
    "            \n",
    "            # Use local Qdrant instance (can be replaced with cloud URL)\n",
    "            self.client = QdrantClient(\":memory:\")  # In-memory for testing; use location=\"./qdrant_data\" for persistence\n",
    "            \n",
    "            # Create collection if it doesn't exist\n",
    "            self.client.recreate_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=self.vector_size,\n",
    "                    distance=models.Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Initialized Qdrant collection: {collection_name}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"qdrant_client not installed. Install with: pip install qdrant-client\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]], embeddings: List[List[float]]):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        from qdrant_client.http import models\n",
    "        \n",
    "        # Prepare points for batch upload\n",
    "        points = []\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            points.append(models.PointStruct(\n",
    "                id=i,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"metadata\": doc[\"metadata\"]\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        # Upload in batch\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        \n",
    "        tracker.end(f\"Adding {len(documents)} documents to vector store\")\n",
    "    \n",
    "    def semantic_search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve documents using semantic similarity\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        search_result = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=top_k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for res in search_result:\n",
    "            results.append({\n",
    "                \"id\": res.id,\n",
    "                \"text\": res.payload[\"text\"],\n",
    "                \"metadata\": res.payload[\"metadata\"],\n",
    "                \"score\": res.score\n",
    "            })\n",
    "        \n",
    "        tracker.end(f\"Semantic search (top-{top_k})\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1ba50-f343-49ee-a97c-84f588f70d9e",
   "metadata": {},
   "source": [
    "##### BM25 LEXICAL SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c256fc4-ca17-43c5-a630-550834fab75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"Implements BM25 for lexical search\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            from rank_bm25 import BM25Okapi\n",
    "            self.BM25Okapi = BM25Okapi\n",
    "            self.bm25 = None\n",
    "            self.documents = []\n",
    "            self.tokenized_corpus = []\n",
    "        except ImportError:\n",
    "            logger.error(\"rank_bm25 not installed. Install with: pip install rank-bm25\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"Index documents for BM25 retrieval\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        self.documents = documents\n",
    "        \n",
    "        # Tokenize and index documents\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        self.tokenized_corpus = [word_tokenize(doc[\"text\"].lower()) for doc in documents]\n",
    "        self.bm25 = self.BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "        tracker.end(f\"Indexing {len(documents)} documents for BM25\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve documents using BM25 lexical similarity\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        from nltk.tokenize import word_tokenize\n",
    "        tokenized_query = word_tokenize(query.lower())\n",
    "        \n",
    "        # Get BM25 scores\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top_k documents\n",
    "        top_indices = doc_scores.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for i in top_indices:\n",
    "            results.append({\n",
    "                \"id\": i,\n",
    "                \"text\": self.documents[i][\"text\"],\n",
    "                \"metadata\": self.documents[i][\"metadata\"],\n",
    "                \"score\": float(doc_scores[i])  # Convert numpy float to Python float\n",
    "            })\n",
    "        \n",
    "        tracker.end(f\"BM25 search (top-{top_k})\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7512-3228-41a0-bdcc-4356b8166819",
   "metadata": {},
   "source": [
    "##### HYBRID RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dce690f-5da0-43dc-8365-e6e66ff42681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Implements hybrid retrieval combining BM25 and embedding search\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_store: VectorStore, \n",
    "        bm25_retriever: BM25Retriever,\n",
    "        embedding_model: EmbeddingModel,\n",
    "        bm25_weight: float = 0.4,\n",
    "        semantic_weight: float = 0.6,\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.embedding_model = embedding_model\n",
    "        self.bm25_weight = bm25_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Perform hybrid retrieval\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        # Get query embedding for semantic search\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        \n",
    "        # Retrieve from both sources (get more results than needed for re-ranking)\n",
    "        retrieval_k = min(top_k * 2, 10)  # Get more docs for re-ranking\n",
    "        semantic_results = self.vector_store.semantic_search(query_embedding, retrieval_k)\n",
    "        bm25_results = self.bm25_retriever.search(query, retrieval_k)\n",
    "        \n",
    "        # Normalize scores from both retrievers (min-max normalization)\n",
    "        def normalize_scores(results):\n",
    "            scores = [r[\"score\"] for r in results]\n",
    "            if not scores:\n",
    "                return results\n",
    "            min_score = min(scores)\n",
    "            max_score = max(scores)\n",
    "            score_range = max_score - min_score\n",
    "            \n",
    "            if score_range > 0:\n",
    "                for r in results:\n",
    "                    r[\"normalized_score\"] = (r[\"score\"] - min_score) / score_range\n",
    "            else:\n",
    "                for r in results:\n",
    "                    r[\"normalized_score\"] = 1.0\n",
    "                    \n",
    "            return results\n",
    "        \n",
    "        semantic_results = normalize_scores(semantic_results)\n",
    "        bm25_results = normalize_scores(bm25_results)\n",
    "        \n",
    "        # Create a unified document pool with combined scores\n",
    "        doc_map = {}\n",
    "        \n",
    "        # Add semantic results\n",
    "        for doc in semantic_results:\n",
    "            doc_id = doc[\"id\"]\n",
    "            doc_map[doc_id] = {\n",
    "                \"id\": doc_id,\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"metadata\": doc[\"metadata\"],\n",
    "                \"semantic_score\": doc[\"normalized_score\"] * self.semantic_weight,\n",
    "                \"bm25_score\": 0,  # Will be updated if in BM25 results\n",
    "                \"combined_score\": doc[\"normalized_score\"] * self.semantic_weight\n",
    "            }\n",
    "        \n",
    "        # Add/update with BM25 results\n",
    "        for doc in bm25_results:\n",
    "            doc_id = doc[\"id\"]\n",
    "            if doc_id in doc_map:\n",
    "                # Update existing entry\n",
    "                doc_map[doc_id][\"bm25_score\"] = doc[\"normalized_score\"] * self.bm25_weight\n",
    "                doc_map[doc_id][\"combined_score\"] += doc[\"normalized_score\"] * self.bm25_weight\n",
    "            else:\n",
    "                # New entry\n",
    "                doc_map[doc_id] = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"metadata\": doc[\"metadata\"],\n",
    "                    \"semantic_score\": 0,  # Not in semantic results\n",
    "                    \"bm25_score\": doc[\"normalized_score\"] * self.bm25_weight,\n",
    "                    \"combined_score\": doc[\"normalized_score\"] * self.bm25_weight\n",
    "                }\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        ranked_results = sorted(\n",
    "            doc_map.values(), \n",
    "            key=lambda x: x[\"combined_score\"], \n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        tracker.end(f\"Hybrid retrieval (top-{top_k})\")\n",
    "        return ranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584fb57-914b-43f4-9e6a-a5bcab204da5",
   "metadata": {},
   "source": [
    "##### LLM INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684a513a-8e68-4a89-9145-af41779cf5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMInterface:\n",
    "    \"\"\"Optimized interface for interacting with LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"mistral-7b-instruct-v0.2.Q3_K_S.gguf\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Determine if we're using a local GGUF model or an API\n",
    "        if model_name.endswith(\".gguf\"):\n",
    "            self._init_local_llm(model_name)\n",
    "        else:\n",
    "            raise ValueError(\"Currently only supporting local GGUF models. Use a .gguf file.\")\n",
    "    \n",
    "    def _init_local_llm(self, model_name: str):\n",
    "        \"\"\"Initialize a local LLM using llama.cpp Python bindings with optimizations\"\"\"\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "            \n",
    "            # Check if model exists in the models directory\n",
    "            model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                logger.warning(f\"Model file not found at {model_path}. Please download it first.\")\n",
    "                logger.info(f\"You can download Mistral 7B Instruct GGUF from: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF\")\n",
    "                model_path = model_name  # Try with just the name in case it's in PATH\n",
    "            \n",
    "            tracker = LatencyTracker().start()\n",
    "            \n",
    "            # Get system memory information\n",
    "            mem_info = psutil.virtual_memory()\n",
    "            available_ram_gb = mem_info.available / (1024**3)\n",
    "            \n",
    "            # Calculate optimal thread count based on CPU cores\n",
    "            # Leave 1-2 cores for the OS\n",
    "            physical_cores = psutil.cpu_count(logical=False) or 4\n",
    "            n_threads = max(1, physical_cores - 1)\n",
    "            \n",
    "            # Determine suitable model layers to GPU\n",
    "            n_gpu_layers = 0  # Keep at 0 for CPU only\n",
    "            \n",
    "            # Load model with optimized parameters for CPU inference\n",
    "            # Use more aggressive quantization (Q3_K_S or Q2_K) for faster inference\n",
    "            self.llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=2048,          # Reduced context window to save memory\n",
    "                n_threads=n_threads,\n",
    "                n_batch=512,         # Batch size for prompt processing\n",
    "                n_gpu_layers=n_gpu_layers,\n",
    "                use_mlock=True,      # Keep model in RAM\n",
    "                use_mmap=True,       # Use memory mapping for faster loading\n",
    "                vocab_only=False,    # Load the full model\n",
    "                seed=-1              # Random seed for reproducibility\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"LLM initialized with {n_threads} threads, context size: 2048\")\n",
    "            tracker.end(f\"Loading optimized LLM model {model_name}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"llama-cpp-python not installed. Install with: pip install llama-cpp-python\")\n",
    "            raise\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    def generate_standalone_question(self, current_question: str, chat_history_key: str) -> str:\n",
    "        \"\"\"Generate a standalone question with caching\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        # Parse chat history from key (needed for caching)\n",
    "        # Format: last 3 turns max, 100 chars max per turn\n",
    "        chat_history = []\n",
    "        turns = chat_history_key.split(\"|||\")\n",
    "        for turn in turns:\n",
    "            if not turn:\n",
    "                continue\n",
    "            parts = turn.split(\":::\")\n",
    "            if len(parts) == 2:\n",
    "                chat_history.append({\"user\": parts[0], \"assistant\": parts[1]})\n",
    "        \n",
    "        # Format chat history for context - use minimal formatting\n",
    "        history_text = \"\"\n",
    "        for turn in chat_history[-2:]:  # Just use last 2 turns for efficiency\n",
    "            if \"user\" in turn:\n",
    "                history_text += f\"User: {turn['user']}\\n\"\n",
    "            if \"assistant\" in turn:\n",
    "                history_text += f\"Assistant: {turn['assistant']}\\n\"\n",
    "        \n",
    "        # Simplified prompt\n",
    "        prompt = f\"\"\"<s>[INST] Based on this conversation:\n",
    "{history_text}\n",
    "Rewrite this follow-up question as a standalone question: {current_question} [/INST]\"\"\"\n",
    "        \n",
    "        # Generate response with optimized parameters\n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_tokens=100,  # Reduced max tokens\n",
    "            temperature=0.0,  # Deterministic output\n",
    "            stop=[\"</s>\"]\n",
    "        )\n",
    "        \n",
    "        standalone_question = response[\"choices\"][0][\"text\"].strip()\n",
    "        \n",
    "        # If the standalone question is too similar to the original, just use original\n",
    "        if len(standalone_question) < 5 or standalone_question == current_question:\n",
    "            standalone_question = current_question\n",
    "        \n",
    "        tracker.end(\"Generating standalone question\")\n",
    "        logger.info(f\"Follow-up: '{current_question}' -> Standalone: '{standalone_question}'\")\n",
    "        \n",
    "        return standalone_question\n",
    "    \n",
    "    def generate_answer(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Generate an answer with optimized prompt\"\"\"\n",
    "        tracker = LatencyTracker().start()\n",
    "        \n",
    "        # Format retrieved documents - only use top 3 and limit text length\n",
    "        context = \"\"\n",
    "        for i, doc in enumerate(retrieved_docs[:3], 1):\n",
    "            source = doc[\"metadata\"].get(\"source\", \"Unknown\")\n",
    "            # Limit each doc to ~500 chars to reduce context size\n",
    "            doc_text = doc[\"text\"]\n",
    "            if len(doc_text) > 500:\n",
    "                doc_text = doc_text[:497] + \"...\"\n",
    "            context += f\"\\nDoc {i} [{source}]: {doc_text}\\n\"\n",
    "        \n",
    "        # Create minimal prompt \n",
    "        prompt = f\"\"\"<s>[INST] Question: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer the question based only on the provided context. Be concise. [/INST]\"\"\"\n",
    "        \n",
    "        # Generate response with optimized parameters\n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_tokens=256,     # Reduced max tokens\n",
    "            temperature=0.1,    # Slight randomness\n",
    "            stop=[\"</s>\"],\n",
    "            top_p=0.9,          # More focused sampling\n",
    "            top_k=40            # Limit vocabulary choices\n",
    "        )\n",
    "        \n",
    "        answer = response[\"choices\"][0][\"text\"].strip()\n",
    "        \n",
    "        tracker.end(\"Generating answer\")\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0df3c-85c2-4ded-aad2-a9e0019319c1",
   "metadata": {},
   "source": [
    "##### RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719a3ae0-61ee-4f07-a555-db32a01dd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system that orchestrates the entire process\"\"\"\n",
    "    \n",
    "    def __init__(self, top_k: int = 3):\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Initialize components\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.text_splitter = TextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "        self.embedding_model = EmbeddingModel(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        self.vector_store = VectorStore(\n",
    "            collection_name=\"documents\",\n",
    "            vector_size=384  # BGE-small embedding dimension\n",
    "        )\n",
    "        self.bm25_retriever = BM25Retriever()\n",
    "        self.hybrid_retriever = HybridRetriever(\n",
    "            vector_store=self.vector_store,\n",
    "            bm25_retriever=self.bm25_retriever,\n",
    "            embedding_model=self.embedding_model,\n",
    "            bm25_weight=0.4,\n",
    "            semantic_weight=0.6,\n",
    "        )\n",
    "        self.llm = LLMInterface(model_name=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\")\n",
    "        \n",
    "        # Chat history for conversational context\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def ingest_documents(self, file_paths: List[str]):\n",
    "        \"\"\"Process and index documents\"\"\"\n",
    "        total_tracker = LatencyTracker().start()\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each document\n",
    "        for file_path in file_paths:\n",
    "            if file_path.lower().endswith(\".pdf\"):\n",
    "                text = self.document_processor.process_pdf(file_path)\n",
    "                chunks = self.text_splitter.split_text(\n",
    "                    text,\n",
    "                    metadata={\"source\": os.path.basename(file_path)}\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported file type: {file_path}\")\n",
    "        \n",
    "        # Extract texts for embedding\n",
    "        texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.embed_texts(texts)\n",
    "        \n",
    "        # Index in both vector store and BM25\n",
    "        self.vector_store.add_documents(all_chunks, embeddings)\n",
    "        self.bm25_retriever.add_documents(all_chunks)\n",
    "        \n",
    "        total_tracker.end(f\"Ingesting {len(file_paths)} documents ({len(all_chunks)} chunks)\")\n",
    "        return len(all_chunks)\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query and return results\"\"\"\n",
    "        total_tracker = LatencyTracker().start()\n",
    "        \n",
    "        # Handle follow-up questions\n",
    "        if self.chat_history:\n",
    "            # Create a hashable key for the chat history (for caching)\n",
    "            history_key = \"|||\".join([\n",
    "                f\"{turn.get('user', '')}:::{turn.get('assistant', '')}\" \n",
    "                for turn in self.chat_history[-3:]  # Only use last 3 turns\n",
    "            ])\n",
    "            standalone_question = self.llm.generate_standalone_question(\n",
    "                question, \n",
    "                history_key\n",
    "            )\n",
    "        else:\n",
    "            standalone_question = question\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.hybrid_retriever.retrieve(\n",
    "            standalone_question, \n",
    "            top_k=self.top_k\n",
    "        )\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.llm.generate_answer(standalone_question, retrieved_docs)\n",
    "        \n",
    "        # Update chat history\n",
    "        self.chat_history.append({\"user\": question})\n",
    "        self.chat_history.append({\"assistant\": answer})\n",
    "\n",
    "        # Limit chat history length to prevent memory growth\n",
    "        if len(self.chat_history) > 10:\n",
    "            self.chat_history = self.chat_history[-10:]\n",
    "        \n",
    "        # Prepare sources information for citation\n",
    "        sources = []\n",
    "        for doc in retrieved_docs:\n",
    "            source = doc[\"metadata\"].get(\"source\", \"Unknown\")\n",
    "            if source not in [s[\"name\"] for s in sources]:\n",
    "                sources.append({\n",
    "                    \"name\": source,\n",
    "                    \"score\": doc[\"combined_score\"]\n",
    "                })\n",
    "        \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"standalone_question\": standalone_question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"latency\": total_tracker.end(\"Total query processing\")\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d1d79-260d-49b7-ae65-7f944b5734fd",
   "metadata": {},
   "source": [
    "##### EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57745e5-9eed-469e-85f9-6356a27c9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluates RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "        try:\n",
    "            import nltk\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "                nltk.data.find('metrics/bleu_ref.txt')\n",
    "            except LookupError:\n",
    "                nltk.download('punkt', quiet=True)\n",
    "                nltk.download('bleu_ref', quiet=True)\n",
    "                \n",
    "            import rouge\n",
    "            self.rouge = rouge.Rouge()\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"Evaluation packages not installed. Install with: pip install nltk rouge-score\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_retrieval(self, query: str, relevant_doc_ids: List[str], retrieved_docs: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval performance using precision, recall, f1\"\"\"\n",
    "        retrieved_ids = [str(doc[\"id\"]) for doc in retrieved_docs]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        true_positives = len(set(retrieved_ids).intersection(set(relevant_doc_ids)))\n",
    "        precision = true_positives / len(retrieved_ids) if retrieved_ids else 0\n",
    "        recall = true_positives / len(relevant_doc_ids) if relevant_doc_ids else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "        }\n",
    "    \n",
    "    def evaluate_generation(self, generated_answer: str, reference_answer: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate answer generation using BLEU and ROUGE\"\"\"\n",
    "        import nltk\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        \n",
    "        # Prepare for BLEU\n",
    "        reference_tokens = [nltk.word_tokenize(reference_answer.lower())]\n",
    "        generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        try:\n",
    "            rouge_scores = self.rouge.get_scores(generated_answer, reference_answer)[0]\n",
    "        except:\n",
    "            rouge_scores = {\"rouge-1\": {\"f\": 0}, \"rouge-2\": {\"f\": 0}, \"rouge-l\": {\"f\": 0}}\n",
    "        \n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"rouge1_f1\": rouge_scores[\"rouge-1\"][\"f\"],\n",
    "            \"rouge2_f1\": rouge_scores[\"rouge-2\"][\"f\"],\n",
    "            \"rougeL_f1\": rouge_scores[\"rouge-l\"][\"f\"],\n",
    "        }\n",
    "    \n",
    "    def evaluate_latency(self, result: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate system latency\"\"\"\n",
    "        return {\n",
    "            \"total_latency\": result[\"latency\"],\n",
    "        }\n",
    "    \n",
    "    def run_benchmark(self, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Run a benchmark on a set of test queries with reference answers\"\"\"\n",
    "        results = {\n",
    "            \"retrieval\": {\n",
    "                \"precision\": [],\n",
    "                \"recall\": [],\n",
    "                \"f1_score\": [],\n",
    "            },\n",
    "            \"generation\": {\n",
    "                \"bleu\": [],\n",
    "                \"rouge1_f1\": [],\n",
    "                \"rouge2_f1\": [],\n",
    "                \"rougeL_f1\": [],\n",
    "            },\n",
    "            \"latency\": {\n",
    "                \"total_latency\": [],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for test in tqdm(test_queries, desc=\"Running benchmark\"):\n",
    "            query = test[\"query\"]\n",
    "            reference_answer = test[\"reference_answer\"]\n",
    "            relevant_doc_ids = test.get(\"relevant_doc_ids\", [])\n",
    "            \n",
    "            # Run query\n",
    "            result = self.rag_system.query(query)\n",
    "            generated_answer = result[\"answer\"]\n",
    "            \n",
    "            # We need to get the retrieved docs from the result for evaluation\n",
    "            # This would require modifying the RAG system to return retrieved docs\n",
    "            # For this example, we'll just use an empty list\n",
    "            retrieved_docs = []  # Placeholder\n",
    "            \n",
    "            # Evaluate\n",
    "            retrieval_metrics = self.evaluate_retrieval(query, relevant_doc_ids, retrieved_docs)\n",
    "            generation_metrics = self.evaluate_generation(generated_answer, reference_answer)\n",
    "            latency_metrics = self.evaluate_latency(result)\n",
    "            \n",
    "            # Collect results\n",
    "            for k, v in retrieval_metrics.items():\n",
    "                results[\"retrieval\"][k].append(v)\n",
    "            for k, v in generation_metrics.items():\n",
    "                results[\"generation\"][k].append(v)\n",
    "            for k, v in latency_metrics.items():\n",
    "                results[\"latency\"][k].append(v)\n",
    "        \n",
    "        # Calculate averages\n",
    "        summary = {\n",
    "            \"retrieval\": {},\n",
    "            \"generation\": {},\n",
    "            \"latency\": {}\n",
    "        }\n",
    "        \n",
    "        for category in results:\n",
    "            for metric in results[category]:\n",
    "                values = results[category][metric]\n",
    "                summary[category][metric] = sum(values) / len(values) if values else 0\n",
    "        \n",
    "        return {\n",
    "            \"details\": results,\n",
    "            \"summary\": summary\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87496fd2-d45c-4e6e-9aef-ed7a9d377335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45205f62-ecb9-4879-978e-cbbf2c6467bd",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ea4dea-3d1b-4d99-9908-69268e0b39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:00:11,676 - rag_system - INFO - Using device: cpu for embeddings\n",
      "2025-03-03 15:00:43,908 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-03-03 15:00:49,997 - rag_system - INFO - Applying dynamic quantization to embedding model\n",
      "2025-03-03 15:00:49,999 - rag_system - WARNING - Quantization failed: 'SentenceTransformer' object has no attribute 'model'\n",
      "2025-03-03 15:00:49,999 - rag_system - INFO - LATENCY: Loading optimized embedding model BAAI/bge-small-en-v1.5 took 6.1812 seconds\n",
      "C:\\Users\\Magrawal\\AppData\\Local\\Temp\\ipykernel_19980\\1932192397.py:16: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  self.client.recreate_collection(\n",
      "2025-03-03 15:00:51,767 - rag_system - INFO - Initialized Qdrant collection: documents\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\Users\\Magrawal\\Downloads\\Experiments\\Humana-poc\\models\\mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "warning: SetProcessWorkingSetSize failed: Insufficient system resources exist to complete the requested service.\n",
      "\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n",
      "2025-03-03 15:00:53,141 - rag_system - INFO - LLM initialized with 9 threads, context size: 2048\n",
      "2025-03-03 15:00:53,141 - rag_system - INFO - LATENCY: Loading optimized LLM model mistral-7b-instruct-v0.2.Q4_K_M.gguf took 1.1741 seconds\n",
      "2025-03-03 15:00:53,314 - pikepdf._core - INFO - pikepdf C++ to Python logger bridge initialized\n",
      "2025-03-03 15:01:19,438 - rag_system - INFO - LATENCY: PDF processing (EveriseHandbook.pdf) took 26.2803 seconds\n",
      "2025-03-03 15:01:19,471 - rag_system - INFO - LATENCY: Text splitting (748 chunks) took 0.0178 seconds\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Aches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.23s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.21s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.20s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A100%|| 1/1 [00:03<00:00,  3.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.29s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.33s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.54s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.28s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.41s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.39s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.43s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.26s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.62s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.49s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.97s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.42s/it]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.32s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.35s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:04<00:00,  4.38s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.40s/it]\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.42s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.50s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.51s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.58s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.62s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.64s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.63s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.94s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.56s/it]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.87s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.95s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:06<00:00,  6.81s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.85s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:06<00:00,  6.71s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:07<00:00,  7.09s/it]\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.75s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.91s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[As: 100%|| 1/1 [00:06<00:00,  6.87s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.90s/it]\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.82s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:06<00:00,  6.84s/it]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.14s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.23s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.30s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.32s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.31s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.30s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.39s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.45s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.55s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.53s/it]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.53s/it]\n",
      "\n",
      "\u001b[Aches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.66s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.77s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.07s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.06s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.08s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|| 1/1 [00:05<00:00,  5.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.24s/it]\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.32s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.23s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.24s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:05<00:00,  5.25s/it]\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.38s/it]\n",
      "Batches: 100%|| 1/1 [00:05<00:00,  5.29s/it]\n",
      "\n",
      "\u001b[Aches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.40s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.37s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.04s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.28s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.09s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.30s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.46s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.29s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.22s/it]\n",
      "\n",
      "\u001b[Aches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.68s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.77s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A  0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.83s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.79s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|| 1/1 [00:03<00:00,  3.73s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.58s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.81s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.67s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.80s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:03<00:00,  3.65s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.68s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.51s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.78s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[As:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.85s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.79s/it]\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.05s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.99s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.13s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A| 1/1 [00:04<00:00,  4.06s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.09s/it]\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.12s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:04<00:00,  4.01s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.77s/it]\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|| 1/1 [00:03<00:00,  3.90s/it]\n",
      "2025-03-03 15:01:56,123 - rag_system - INFO - LATENCY: Embedding 748 texts took 36.6522 seconds\n",
      "2025-03-03 15:01:56,679 - rag_system - INFO - LATENCY: Adding 748 documents to vector store took 0.5556 seconds\n",
      "2025-03-03 15:01:57,646 - rag_system - INFO - LATENCY: Indexing 748 documents for BM25 took 0.9547 seconds\n",
      "2025-03-03 15:01:57,646 - rag_system - INFO - LATENCY: Ingesting 1 documents (748 chunks) took 64.4887 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RAG Conversational System Demo =====\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  tell me about the policies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00,  8.91it/s]\n",
      "2025-03-03 15:08:40,242 - rag_system - INFO - LATENCY: Query embedding took 0.1123 seconds\n",
      "C:\\Users\\Magrawal\\AppData\\Local\\Temp\\ipykernel_19980\\1932192397.py:60: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = self.client.search(\n",
      "2025-03-03 15:08:40,262 - rag_system - INFO - LATENCY: Semantic search (top-6) took 0.0200 seconds\n",
      "2025-03-03 15:08:40,262 - rag_system - INFO - LATENCY: BM25 search (top-6) took 0.0000 seconds\n",
      "2025-03-03 15:08:40,262 - rag_system - INFO - LATENCY: Hybrid retrieval (top-3) took 0.1322 seconds\n",
      "C:\\Users\\Magrawal\\Downloads\\Experiments\\Humana-poc\\humanaVenv\\lib\\site-packages\\llama_cpp\\llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "llama_perf_context_print:        load time =   57931.29 ms\n",
      "llama_perf_context_print: prompt eval time =   57923.70 ms /   375 tokens (  154.46 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20856.83 ms /    62 runs   (  336.40 ms per token,     2.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   78852.93 ms /   437 tokens\n",
      "2025-03-03 15:09:59,167 - rag_system - INFO - LATENCY: Generating answer took 78.9051 seconds\n",
      "2025-03-03 15:09:59,169 - rag_system - INFO - LATENCY: Total query processing took 79.0394 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Conflict of Interest Policy is a document outlining the guidelines for Everise employees to identify and manage potential conflicts of interest between their personal interests and their duties at the company. The policy is subject to change and employees are encouraged to seek clarification from the Chief Legal Officer if they have any questions.\n",
      "\n",
      "Sources:\n",
      "- EveriseHandbook.pdf (score: 0.600)\n",
      "\n",
      "Latency: 79.039 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage of the RAG system\"\"\"\n",
    "    \n",
    "    # Initialize the system\n",
    "    rag = RAGSystem(top_k=3)\n",
    "    \n",
    "    # Ingest documents (example paths)\n",
    "    document_paths = [\n",
    "        \"Documents/EveriseHandbook.pdf\",\n",
    "    ]\n",
    "    \n",
    "    # Check if documents exist\n",
    "    existing_docs = [path for path in document_paths if os.path.exists(path)]\n",
    "    if not existing_docs:\n",
    "        logger.warning(f\"No documents found at the specified paths. Skipping ingestion.\")\n",
    "    else:\n",
    "        rag.ingest_documents(existing_docs)\n",
    "    \n",
    "    # Interactive querying\n",
    "    print(\"\\n===== RAG Conversational System Demo =====\")\n",
    "    print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \")\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "            \n",
    "        print(\"\\nProcessing...\")\n",
    "        \n",
    "        try:\n",
    "            result = rag.query(question)\n",
    "            \n",
    "            print(f\"\\nAnswer: {result['answer']}\")\n",
    "            print(f\"\\nSources:\")\n",
    "            for source in result['sources']:\n",
    "                print(f\"- {source['name']} (score: {source['score']:.3f})\")\n",
    "            print(f\"\\nLatency: {result['latency']:.3f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc45c0d-7873-4787-a131-a87916daf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-Evaluation Module for RAG System\n",
    "- Automatically generates test queries\n",
    "- Creates synthetic reference answers\n",
    "- Evaluates system without external data\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"rag_system\")\n",
    "\n",
    "class RAGSelfEvaluator:\n",
    "    \"\"\"Self-evaluation for RAG systems without external datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "        try:\n",
    "            import nltk\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "                nltk.data.find('metrics/bleu_ref.txt')\n",
    "            except LookupError:\n",
    "                nltk.download('punkt', quiet=True)\n",
    "                nltk.download('bleu_ref', quiet=True)\n",
    "                \n",
    "            import rouge\n",
    "            self.rouge = rouge.Rouge()\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"Evaluation packages not installed. Install with: pip install nltk rouge-score\")\n",
    "            raise\n",
    "    \n",
    "    def generate_test_queries(self, num_queries: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate test queries based on documents in the system\"\"\"\n",
    "        # Get all documents from the system\n",
    "        # This assumes we can access the documents - we'll extract from BM25 retriever\n",
    "        if not hasattr(self.rag_system, 'bm25_retriever') or not self.rag_system.bm25_retriever.documents:\n",
    "            logger.warning(\"No documents found in the system for query generation\")\n",
    "            return []\n",
    "        \n",
    "        all_docs = self.rag_system.bm25_retriever.documents\n",
    "        if not all_docs:\n",
    "            logger.warning(\"No documents found in the system for query generation\")\n",
    "            return []\n",
    "            \n",
    "        test_queries = []\n",
    "        \n",
    "        # Extract key sentences from random documents\n",
    "        for _ in range(num_queries):\n",
    "            # Pick a random document\n",
    "            doc = random.choice(all_docs)\n",
    "            doc_text = doc[\"text\"]\n",
    "            \n",
    "            # Split into sentences\n",
    "            import nltk\n",
    "            sentences = nltk.sent_tokenize(doc_text)\n",
    "            \n",
    "            if not sentences:\n",
    "                continue\n",
    "                \n",
    "            # Pick a random sentence as the base for our query\n",
    "            sentence = random.choice(sentences)\n",
    "            \n",
    "            # Create a question from the sentence\n",
    "            question_types = [\n",
    "                \"What is \", \"How does \", \"Can you explain \", \n",
    "                \"Tell me about \", \"Why is \", \"What are \"\n",
    "            ]\n",
    "            \n",
    "            # Extract key terms (nouns) from the sentence\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # Find nouns\n",
    "            nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "            \n",
    "            if not nouns:\n",
    "                # If no nouns, just use the whole sentence\n",
    "                subject = sentence\n",
    "            else:\n",
    "                # Use a random noun as the subject\n",
    "                subject = random.choice(nouns)\n",
    "            \n",
    "            # Generate the question\n",
    "            question = random.choice(question_types) + subject + \"?\"\n",
    "            \n",
    "            # The source document will be our \"relevant\" document\n",
    "            relevant_doc_id = doc[\"id\"]\n",
    "            \n",
    "            # Use the sentence as our \"reference answer\"\n",
    "            reference_answer = sentence\n",
    "            \n",
    "            test_queries.append({\n",
    "                \"query\": question,\n",
    "                \"relevant_doc_ids\": [str(relevant_doc_id)],\n",
    "                \"reference_answer\": reference_answer,\n",
    "                \"source_doc\": doc[\"id\"],\n",
    "                \"source_text\": doc_text[:200] + \"...\" # First 200 chars for reference\n",
    "            })\n",
    "        \n",
    "        return test_queries\n",
    "    \n",
    "    def evaluate_query(self, query: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a single query\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the query\n",
    "        result = self.rag_system.query(query[\"query\"])\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        try:\n",
    "            rouge_scores = self.rouge.get_scores(result[\"answer\"], query[\"reference_answer\"])[0]\n",
    "        except:\n",
    "            rouge_scores = {\"rouge-1\": {\"f\": 0}, \"rouge-2\": {\"f\": 0}, \"rouge-l\": {\"f\": 0}}\n",
    "        \n",
    "        # Check if source document was retrieved\n",
    "        # We can't directly check this without modifying the RAG system\n",
    "        # So we'll check if any key phrases from the source appear in the answer\n",
    "        source_text = query[\"source_text\"]\n",
    "        key_phrases = nltk.word_tokenize(source_text)[:20]  # First 20 tokens\n",
    "        \n",
    "        # Count how many key phrases appear in the answer\n",
    "        answer_tokens = nltk.word_tokenize(result[\"answer\"].lower())\n",
    "        key_phrase_overlap = sum(1 for token in key_phrases if token.lower() in answer_tokens)\n",
    "        retrieval_score = key_phrase_overlap / max(1, len(key_phrases))\n",
    "        \n",
    "        evaluation = {\n",
    "            \"query\": query[\"query\"],\n",
    "            \"reference_answer\": query[\"reference_answer\"],\n",
    "            \"generated_answer\": result[\"answer\"],\n",
    "            \"latency\": latency,\n",
    "            \"rouge1_f1\": rouge_scores[\"rouge-1\"][\"f\"],\n",
    "            \"rouge2_f1\": rouge_scores[\"rouge-2\"][\"f\"], \n",
    "            \"rougeL_f1\": rouge_scores[\"rouge-l\"][\"f\"],\n",
    "            \"retrieval_score\": retrieval_score,\n",
    "            \"sources\": result.get(\"sources\", [])\n",
    "        }\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    def run_self_evaluation(self, num_queries: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Run a complete self-evaluation\"\"\"\n",
    "        # Generate test queries\n",
    "        logger.info(f\"Generating {num_queries} test queries\")\n",
    "        test_queries = self.generate_test_queries(num_queries)\n",
    "        \n",
    "        if not test_queries:\n",
    "            return {\"error\": \"Could not generate test queries. No documents available.\"}\n",
    "        \n",
    "        # Evaluate each query\n",
    "        results = []\n",
    "        for query in test_queries:\n",
    "            logger.info(f\"Evaluating query: {query['query']}\")\n",
    "            result = self.evaluate_query(query)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Calculate average scores\n",
    "        avg_latency = sum(r[\"latency\"] for r in results) / len(results)\n",
    "        avg_rouge1 = sum(r[\"rouge1_f1\"] for r in results) / len(results)\n",
    "        avg_rouge2 = sum(r[\"rouge2_f1\"] for r in results) / len(results)\n",
    "        avg_rougeL = sum(r[\"rougeL_f1\"] for r in results) / len(results)\n",
    "        avg_retrieval = sum(r[\"retrieval_score\"] for r in results) / len(results)\n",
    "        \n",
    "        summary = {\n",
    "            \"num_queries\": len(results),\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"avg_rouge1_f1\": avg_rouge1,\n",
    "            \"avg_rouge2_f1\": avg_rouge2,\n",
    "            \"avg_rougeL_f1\": avg_rougeL,\n",
    "            \"avg_retrieval_score\": avg_retrieval,\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        # Log summary\n",
    "        logger.info(\"Evaluation Summary:\")\n",
    "        logger.info(f\"  Number of Queries: {summary['num_queries']}\")\n",
    "        logger.info(f\"  Average Latency: {summary['avg_latency']:.2f} seconds\")\n",
    "        logger.info(f\"  Average ROUGE-1 F1: {summary['avg_rouge1_f1']:.4f}\")\n",
    "        logger.info(f\"  Average ROUGE-L F1: {summary['avg_rougeL_f1']:.4f}\")\n",
    "        logger.info(f\"  Average Retrieval Score: {summary['avg_retrieval_score']:.4f}\")\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d90f1-ab38-46ec-883d-673d783596c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
